=== Host-Side Parallelism Calculation ===
Host Cores: 32
Target Jobs (80%): 25
Detected Arch: CPU=znver5, GPU=gfx1151
=== Cleaning up old containers ===
=== AMD AI Builder: Initializing Docker Infrastructure ===
Building amd-ai-builder:local...
#0 building with "default" instance using docker driver

#1 [internal] load build definition from Dockerfile
#1 transferring dockerfile: 1.58kB done
#1 DONE 0.0s

#2 [internal] load metadata for docker.io/rocm/dev-ubuntu-24.04:7.1.1-complete
#2 DONE 0.2s

#3 [internal] load .dockerignore
#3 transferring context: 2B done
#3 DONE 0.0s

#4 [1/6] FROM docker.io/rocm/dev-ubuntu-24.04:7.1.1-complete@sha256:c6648f6a60470959f5f9c653ce8397d72fc0adda455942b265a5f973c9ee5891
#4 DONE 0.0s

#5 [3/6] RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-14 100 --slave /usr/bin/g++ g++ /usr/bin/g++-14 &&     update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 &&     update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1
#5 CACHED

#6 [4/6] RUN python3.11 -m pip install --upgrade pip --break-system-packages || true
#6 CACHED

#7 [5/6] RUN python3.11 -m pip install --break-system-packages     ninja meson meson-python cython pybind11     setuptools wheel packaging pyyaml typing-extensions     sympy mpmath requests psutil tqdm || true
#7 CACHED

#8 [2/6] RUN apt-get update && apt-get install -y     software-properties-common     && add-apt-repository ppa:deadsnakes/ppa     && apt-get update && apt-get install -y     python3.11     python3.11-venv     python3.11-dev     python3.11-full     python3-pip     gcc-14     g++-14     git     cmake     ninja-build     build-essential     wget     libopenblas-dev     libjpeg-dev     zlib1g-dev     libpng-dev     libtiff-dev     libfreetype6-dev     liblcms2-dev     libwebp-dev     liblzma-dev     libffi-dev     pkg-config     && rm -rf /var/lib/apt/lists/*
#8 CACHED

#9 [6/6] WORKDIR /app
#9 CACHED

#10 exporting to image
#10 exporting layers done
#10 writing image sha256:f498e10c580be4932cf8700b38b3e694252780f8846855c09b3d19bbeb7b0213 done
#10 naming to docker.io/library/amd-ai-builder:local done
#10 DONE 0.0s
=== AMD AI Builder: Starting Containerized Build Pipeline ===
Running prefetch stage...
=== AMD AI Builder: Starting Comprehensive Prefetch Stage ===
>>> Syncing Git repositories...
Optimizing git clone/submodule throughput (jobs=15)...
>>> Updating /home/nexus/amd-ai/src/extras/triton-rocm...
From https://github.com/triton-lang/triton
 * tag                   v3.1.0     -> FETCH_HEAD
HEAD is now at cf34004b8 AMD requested cherry-picks for release/3.1.x (#4794)
M	CMakeLists.txt
M	lib/Conversion/TritonGPUToLLVM/CMakeLists.txt
M	python/setup.py
From https://github.com/triton-lang/triton
 * tag                   v3.1.0     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/torchvision...
From https://github.com/pytorch/vision
 * tag                     v0.20.1    -> FETCH_HEAD
HEAD is now at 3ac97aa912 Update version.txt to 0.20.1 (#8694)
From https://github.com/pytorch/vision
 * tag                     v0.20.1    -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/torchaudio...
From https://github.com/pytorch/audio
 * tag                 v2.5.1     -> FETCH_HEAD
HEAD is now at 1661daf1 Release 2.5.1 update version (#3847)
From https://github.com/pytorch/audio
 * tag                 v2.5.1     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/numpy...
From https://github.com/numpy/numpy
 * tag                     v2.2.1     -> FETCH_HEAD
HEAD is now at 7469245786 Merge pull request #28047 from charris/prepare-2.2.1
M	vendored-meson/meson
From https://github.com/numpy/numpy
 * tag                     v2.2.1     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/flash-attention...
From https://github.com/ROCm/flash-attention
 * tag                 v2.7.4-cktile -> FETCH_HEAD
HEAD is now at e9e96d3d Sync the compile flag with CK (#1670)
From https://github.com/ROCm/flash-attention
 * tag                 v2.7.4-cktile -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/xformers...
From https://github.com/facebookresearch/xformers
 * tag                 v0.0.29    -> FETCH_HEAD
HEAD is now at 56be3b5e Changelog for 0.0.29 (fairinternal/xformers#1275)
M	third_party/composable_kernel_tiled
From https://github.com/facebookresearch/xformers
 * tag                 v0.0.29    -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/bitsandbytes...
From https://github.com/ROCm/bitsandbytes
 * branch            rocm_enabled -> FETCH_HEAD
Already on 'rocm_enabled'
Your branch is up to date with 'origin/rocm_enabled'.
From https://github.com/ROCm/bitsandbytes
 * branch            rocm_enabled -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/deepspeed...
From https://github.com/microsoft/DeepSpeed
 * tag                 v0.16.2    -> FETCH_HEAD
HEAD is now at b344c04d Update code owners (#6890)
From https://github.com/microsoft/DeepSpeed
 * tag                 v0.16.2    -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/onnxruntime...
From https://github.com/microsoft/onnxruntime
 * tag                     v1.20.1    -> FETCH_HEAD
HEAD is now at 5c1b7ccbff [ORT 1.20.1 Release] Cherry pick 2nd round (#22845)
D	js/react_native/e2e/yarn.lock
D	js/react_native/yarn.lock
From https://github.com/microsoft/onnxruntime
 * tag                     v1.20.1    -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/cupy...
From https://github.com/cupy/cupy
 * tag                   v13.3.0    -> FETCH_HEAD
HEAD is now at 118ade4a1 Merge pull request #8548 from kmaehashi/bump-docker-v13.3.0
From https://github.com/cupy/cupy
 * tag                   v13.3.0    -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/faiss...
From https://github.com/facebookresearch/faiss
 * tag                   v1.9.0     -> FETCH_HEAD
HEAD is now at d243e6288 Updated conda CI label from staging to main, INSTALL.md (#3929)
From https://github.com/facebookresearch/faiss
 * tag                   v1.9.0     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/opencv...
From https://github.com/opencv/opencv
 * tag                     4.10.0     -> FETCH_HEAD
HEAD is now at 71d3237a09 Release 4.10.0
From https://github.com/opencv/opencv
 * tag                     4.10.0     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/opencv_contrib...
From https://github.com/opencv/opencv_contrib
 * tag                 4.10.0     -> FETCH_HEAD
HEAD is now at 1ed3dd2c Merge pull request #3744 from asmorkalov:as/variadic_tuple
From https://github.com/opencv/opencv_contrib
 * tag                 4.10.0     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/pillow-simd...
From https://github.com/uploadcare/pillow-simd
 * tag                   10.4.0     -> FETCH_HEAD
HEAD is now at 9b4fae771 10.4.0 version bump
From https://github.com/uploadcare/pillow-simd
 * tag                   10.4.0     -> FETCH_HEAD
Already up to date.
>>> Updating /home/nexus/amd-ai/src/extras/llama-cpp...
From https://github.com/ggml-org/llama.cpp
 * tag                   b7551      -> FETCH_HEAD
HEAD is now at a52dc60ba llama_fit_params: return enum for fail vs. error (#18374)
D	flake.lock
D	poetry.lock
From https://github.com/ggml-org/llama.cpp
 * tag                   b7551      -> FETCH_HEAD
Already up to date.
Prefetching submodules/objects: /home/nexus/amd-ai/src/pytorch
From https://github.com/pytorch/pytorch
 * branch                    HEAD                        -> FETCH_HEAD
 ! [rejected]                ciflow/b200/115316          -> ciflow/b200/115316  (would clobber existing tag)
 ! [rejected]                ciflow/b200/161607          -> ciflow/b200/161607  (would clobber existing tag)
 ! [rejected]                ciflow/b200/170832          -> ciflow/b200/170832  (would clobber existing tag)
 ! [rejected]                ciflow/b200/171011          -> ciflow/b200/171011  (would clobber existing tag)
 * [new tag]                 ciflow/b200/171205          -> ciflow/b200/171205
 ! [rejected]                ciflow/h100-symm-mem/171203 -> ciflow/h100-symm-mem/171203  (would clobber existing tag)
 ! [rejected]                ciflow/h100/115316          -> ciflow/h100/115316  (would clobber existing tag)
 ! [rejected]                ciflow/h100/161607          -> ciflow/h100/161607  (would clobber existing tag)
 ! [rejected]                ciflow/h100/170832          -> ciflow/h100/170832  (would clobber existing tag)
 ! [rejected]                ciflow/h100/171011          -> ciflow/h100/171011  (would clobber existing tag)
 * [new tag]                 ciflow/h100/171205          -> ciflow/h100/171205
 ! [rejected]                ciflow/inductor-perf-test-nightly-rocm-mi300/168073 -> ciflow/inductor-perf-test-nightly-rocm-mi300/168073  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/146506      -> ciflow/inductor/146506  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/147990      -> ciflow/inductor/147990  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/148492      -> ciflow/inductor/148492  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/166063      -> ciflow/inductor/166063  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/166741      -> ciflow/inductor/166741  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/167407      -> ciflow/inductor/167407  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/167583      -> ciflow/inductor/167583  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/167680      -> ciflow/inductor/167680  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/168073      -> ciflow/inductor/168073  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/168952      -> ciflow/inductor/168952  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169023      -> ciflow/inductor/169023  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169121      -> ciflow/inductor/169121  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169260      -> ciflow/inductor/169260  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169282      -> ciflow/inductor/169282  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169415      -> ciflow/inductor/169415  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169432      -> ciflow/inductor/169432  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169493      -> ciflow/inductor/169493  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169513      -> ciflow/inductor/169513  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169548      -> ciflow/inductor/169548  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/169801      -> ciflow/inductor/169801  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/170832      -> ciflow/inductor/170832  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/170880      -> ciflow/inductor/170880  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171011      -> ciflow/inductor/171011  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171131      -> ciflow/inductor/171131  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171192      -> ciflow/inductor/171192  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171205      -> ciflow/inductor/171205  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171229      -> ciflow/inductor/171229  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171291      -> ciflow/inductor/171291  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171328      -> ciflow/inductor/171328  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171358      -> ciflow/inductor/171358  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171360      -> ciflow/inductor/171360  (would clobber existing tag)
 ! [rejected]                ciflow/inductor/171457      -> ciflow/inductor/171457  (would clobber existing tag)
 ! [rejected]                ciflow/mps/169018           -> ciflow/mps/169018  (would clobber existing tag)
 ! [rejected]                ciflow/rocm-mi300/115316    -> ciflow/rocm-mi300/115316  (would clobber existing tag)
 ! [rejected]                ciflow/rocm-mi300/161607    -> ciflow/rocm-mi300/161607  (would clobber existing tag)
 ! [rejected]                ciflow/rocm-mi300/170832    -> ciflow/rocm-mi300/170832  (would clobber existing tag)
 ! [rejected]                ciflow/rocm-mi300/171011    -> ciflow/rocm-mi300/171011  (would clobber existing tag)
 * [new tag]                 ciflow/rocm-mi300/171205    -> ciflow/rocm-mi300/171205
 ! [rejected]                ciflow/rocm/115316          -> ciflow/rocm/115316  (would clobber existing tag)
 ! [rejected]                ciflow/rocm/148492          -> ciflow/rocm/148492  (would clobber existing tag)
 ! [rejected]                ciflow/rocm/161607          -> ciflow/rocm/161607  (would clobber existing tag)
 ! [rejected]                ciflow/rocm/168073          -> ciflow/rocm/168073  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/148492         -> ciflow/trunk/148492  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/166741         -> ciflow/trunk/166741  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/169023         -> ciflow/trunk/169023  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/169260         -> ciflow/trunk/169260  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/169442         -> ciflow/trunk/169442  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/169559         -> ciflow/trunk/169559  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/170832         -> ciflow/trunk/170832  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/170864         -> ciflow/trunk/170864  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/170865         -> ciflow/trunk/170865  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/171270         -> ciflow/trunk/171270  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/171313         -> ciflow/trunk/171313  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/171446         -> ciflow/trunk/171446  (would clobber existing tag)
 ! [rejected]                ciflow/trunk/171453         -> ciflow/trunk/171453  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/166843           -> ciflow/xpu/166843  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/169039           -> ciflow/xpu/169039  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/169442           -> ciflow/xpu/169442  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/169559           -> ciflow/xpu/169559  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/171291           -> ciflow/xpu/171291  (would clobber existing tag)
 ! [rejected]                ciflow/xpu/171453           -> ciflow/xpu/171453  (would clobber existing tag)
Fetch warning on /home/nexus/amd-ai/src/pytorch
Synchronizing submodule url for 'android/libs/fbjni'
Synchronizing submodule url for 'third_party/FP16'
Synchronizing submodule url for 'third_party/FXdiv'
Synchronizing submodule url for 'third_party/NNPACK'
Synchronizing submodule url for 'third_party/NVTX'
Synchronizing submodule url for 'third_party/VulkanMemoryAllocator'
Synchronizing submodule url for 'third_party/XNNPACK'
Synchronizing submodule url for 'third_party/aiter'
Synchronizing submodule url for 'third_party/aiter/3rdparty/composable_kernel'
Synchronizing submodule url for 'third_party/benchmark'
Synchronizing submodule url for 'third_party/composable_kernel'
Synchronizing submodule url for 'third_party/cpp-httplib'
Synchronizing submodule url for 'third_party/cpuinfo'
Synchronizing submodule url for 'third_party/cudnn_frontend'
Synchronizing submodule url for 'third_party/cutlass'
Synchronizing submodule url for 'third_party/fbgemm'
Synchronizing submodule url for 'third_party/fbgemm/external/asmjit'
Synchronizing submodule url for 'third_party/fbgemm/external/composable_kernel'
Synchronizing submodule url for 'third_party/fbgemm/external/cpuinfo'
Synchronizing submodule url for 'third_party/fbgemm/external/cutlass'
Synchronizing submodule url for 'third_party/fbgemm/external/googletest'
Synchronizing submodule url for 'third_party/fbgemm/external/hipify_torch'
Synchronizing submodule url for 'third_party/fbgemm/external/json'
Synchronizing submodule url for 'third_party/flash-attention'
Synchronizing submodule url for 'third_party/flash-attention/csrc/composable_kernel'
Synchronizing submodule url for 'third_party/flash-attention/csrc/cutlass'
Synchronizing submodule url for 'third_party/flatbuffers'
Synchronizing submodule url for 'third_party/fmt'
Synchronizing submodule url for 'third_party/gemmlowp/gemmlowp'
Synchronizing submodule url for 'third_party/gloo'
Synchronizing submodule url for 'third_party/googletest'
Synchronizing submodule url for 'third_party/ideep'
Synchronizing submodule url for 'third_party/ideep/mkl-dnn'
Synchronizing submodule url for 'third_party/ittapi'
Synchronizing submodule url for 'third_party/kineto'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/DCGM'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/cpr'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/fmt'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/gflags'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/gflags/doc'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/glog'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/googletest'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/json'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/dynolog/third_party/pfs'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/fmt'
Synchronizing submodule url for 'third_party/kineto/libkineto/third_party/googletest'
Synchronizing submodule url for 'third_party/kleidiai'
Synchronizing submodule url for 'third_party/mimalloc'
Synchronizing submodule url for 'third_party/nlohmann'
Synchronizing submodule url for 'third_party/onnx'
Synchronizing submodule url for 'third_party/onnx/third_party/pybind11'
Synchronizing submodule url for 'third_party/opentelemetry-cpp'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/benchmark'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/googletest'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/ms-gsl'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/nlohmann-json'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/opentelemetry-proto'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/opentracing-cpp'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/prometheus-cpp'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/prometheus-cpp/3rdparty/civetweb'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/third_party/prometheus-cpp/3rdparty/googletest'
Synchronizing submodule url for 'third_party/opentelemetry-cpp/tools/vcpkg'
Synchronizing submodule url for 'third_party/pocketfft'
Synchronizing submodule url for 'third_party/protobuf'
Synchronizing submodule url for 'third_party/protobuf/third_party/benchmark'
Synchronizing submodule url for 'third_party/protobuf/third_party/googletest'
Synchronizing submodule url for 'third_party/psimd'
Synchronizing submodule url for 'third_party/pthreadpool'
Synchronizing submodule url for 'third_party/pybind11'
Synchronizing submodule url for 'third_party/python-peachpy'
Synchronizing submodule url for 'third_party/sleef'
Synchronizing submodule url for 'third_party/tensorpipe'
Synchronizing submodule url for 'third_party/tensorpipe/third_party/googletest'
Synchronizing submodule url for 'third_party/tensorpipe/third_party/libnop'
Synchronizing submodule url for 'third_party/tensorpipe/third_party/libuv'
Synchronizing submodule url for 'third_party/tensorpipe/third_party/pybind11'
Synchronizing submodule url for 'third_party/tensorpipe/third_party/pybind11/tools/clang'
Prefetching submodules/objects: /home/nexus/amd-ai/src/vllm
From https://github.com/vllm-project/vllm
 * branch                HEAD       -> FETCH_HEAD
Prefetching submodules/objects: /home/nexus/amd-ai/src/llama.cpp
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/triton-rocm
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/torchvision
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/torchaudio
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/numpy
Fetching submodule doc/source/_static/scipy-mathjax
Fetching submodule numpy/_core/src/highway
Fetching submodule numpy/_core/src/common/pythoncapi-compat
Fetching submodule numpy/_core/src/npysort/x86-simd-sort
Fetching submodule numpy/_core/src/umath/svml
Fetching submodule numpy/fft/pocketfft
Fetching submodule vendored-meson/meson
fatal: Unable to create '/home/nexus/amd-ai/src/extras/numpy/.git/objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Fetch warning on /home/nexus/amd-ai/src/extras/numpy
Synchronizing submodule url for 'doc/source/_static/scipy-mathjax'
Synchronizing submodule url for 'numpy/_core/src/common/pythoncapi-compat'
Synchronizing submodule url for 'numpy/_core/src/highway'
Synchronizing submodule url for 'numpy/_core/src/npysort/x86-simd-sort'
Synchronizing submodule url for 'numpy/_core/src/umath/svml'
Synchronizing submodule url for 'numpy/fft/pocketfft'
Synchronizing submodule url for 'vendored-meson/meson'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/flash-attention
Fetching submodule csrc/composable_kernel
Fetching submodule csrc/cutlass
fatal: Unable to create '/home/nexus/amd-ai/src/extras/flash-attention/.git/objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Fetch warning on /home/nexus/amd-ai/src/extras/flash-attention
Synchronizing submodule url for 'csrc/composable_kernel'
Synchronizing submodule url for 'csrc/cutlass'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/xformers
Fetching submodule third_party/composable_kernel_tiled
Fetching submodule third_party/cutlass
Fetching submodule third_party/flash-attention
Fetching submodule third_party/flash-attention/csrc/composable_kernel
Fetching submodule third_party/flash-attention/csrc/cutlass
fatal: Unable to create '/home/nexus/amd-ai/src/extras/xformers/.git/modules/third_party/flash-attention/./objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Errors during submodule fetch:
	third_party/flash-attention
fatal: Unable to create '/home/nexus/amd-ai/src/extras/xformers/.git/objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Fetch warning on /home/nexus/amd-ai/src/extras/xformers
Synchronizing submodule url for 'third_party/composable_kernel_tiled'
Synchronizing submodule url for 'third_party/cutlass'
Synchronizing submodule url for 'third_party/flash-attention'
Synchronizing submodule url for 'third_party/flash-attention/csrc/composable_kernel'
Synchronizing submodule url for 'third_party/flash-attention/csrc/cutlass'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/bitsandbytes
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/deepspeed
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/onnxruntime
Fetching submodule cmake/external/emsdk
Fetching submodule cmake/external/libprotobuf-mutator
Fetching submodule cmake/external/onnx
Fetching submodule cmake/external/onnx/third_party/benchmark
Fetching submodule cmake/external/onnx/third_party/pybind11
fatal: Unable to create '/home/nexus/amd-ai/src/extras/onnxruntime/.git/modules/cmake/external/onnx/./objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Errors during submodule fetch:
	cmake/external/onnx
/home/nexus/amd-ai/scripts/05_git_parallel_prefetch.sh: line 98: 1848124 Segmentation fault      (core dumped) git -c protocol.version=2 fetch --all --tags --recurse-submodules -j "$GIT_JOBS" --prune
Fetch warning on /home/nexus/amd-ai/src/extras/onnxruntime
Synchronizing submodule url for 'cmake/external/emsdk'
Synchronizing submodule url for 'cmake/external/libprotobuf-mutator'
Synchronizing submodule url for 'cmake/external/onnx'
Synchronizing submodule url for 'cmake/external/onnx/third_party/benchmark'
Synchronizing submodule url for 'cmake/external/onnx/third_party/pybind11'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/cupy
Fetching submodule third_party/cccl
Fetching submodule third_party/dlpack
Fetching submodule third_party/jitify
fatal: Unable to create '/home/nexus/amd-ai/src/extras/cupy/.git/objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Fetch warning on /home/nexus/amd-ai/src/extras/cupy
Synchronizing submodule url for 'third_party/cccl'
Synchronizing submodule url for 'third_party/dlpack'
Synchronizing submodule url for 'third_party/jitify'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/faiss
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/opencv
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/opencv_contrib
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/pillow-simd
Fetching submodule wheels/multibuild
fatal: Unable to create '/home/nexus/amd-ai/src/extras/pillow-simd/.git/objects/info/commit-graphs/commit-graph-chain.lock': File exists.

Another git process seems to be running in this repository, e.g.
an editor opened by 'git commit'. Please make sure all processes
are terminated then try again. If it still fails, a git process
may have crashed in this repository earlier:
remove the file manually to continue.
Fetch warning on /home/nexus/amd-ai/src/extras/pillow-simd
Synchronizing submodule url for 'wheels/multibuild'
Prefetching submodules/objects: /home/nexus/amd-ai/src/extras/llama-cpp
Done. Git repositories are synchronized.
>>> Downloading Python wheels to /home/nexus/amd-ai/wheels/cache...
Collecting pip
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pip-25.3-py3-none-any.whl
Collecting setuptools
  File was already downloaded /home/nexus/amd-ai/wheels/cache/setuptools-80.9.0-py3-none-any.whl
Collecting wheel
  File was already downloaded /home/nexus/amd-ai/wheels/cache/wheel-0.45.1-py3-none-any.whl
Collecting ninja
  File was already downloaded /home/nexus/amd-ai/wheels/cache/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting cmake
  File was already downloaded /home/nexus/amd-ai/wheels/cache/cmake-4.2.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting packaging
  File was already downloaded /home/nexus/amd-ai/wheels/cache/packaging-25.0-py3-none-any.whl
Collecting pybind11
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pybind11-3.0.1-py3-none-any.whl
Collecting swig
  File was already downloaded /home/nexus/amd-ai/wheels/cache/swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl
Collecting meson
  File was already downloaded /home/nexus/amd-ai/wheels/cache/meson-1.10.0-py3-none-any.whl
Collecting meson-python
  File was already downloaded /home/nexus/amd-ai/wheels/cache/meson_python-0.18.0-py3-none-any.whl
Collecting cython
  File was already downloaded /home/nexus/amd-ai/wheels/cache/cython-3.2.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting transformers>=4.56.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/transformers-4.57.3-py3-none-any.whl
Collecting accelerate
  File was already downloaded /home/nexus/amd-ai/wheels/cache/accelerate-1.12.0-py3-none-any.whl
Collecting setuptools-scm>=8
  File was already downloaded /home/nexus/amd-ai/wheels/cache/setuptools_scm-9.2.2-py3-none-any.whl
Collecting sentencepiece
  File was already downloaded /home/nexus/amd-ai/wheels/cache/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting protobuf
  File was already downloaded /home/nexus/amd-ai/wheels/cache/protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl
Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0)
  File was already downloaded /home/nexus/amd-ai/wheels/cache/fastapi-0.128.0-py3-none-any.whl
Collecting aiohttp
  File was already downloaded /home/nexus/amd-ai/wheels/cache/aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting openai>=1.99.1
  File was already downloaded /home/nexus/amd-ai/wheels/cache/openai-2.14.0-py3-none-any.whl
Collecting pydantic>=2.12.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pydantic-2.12.5-py3-none-any.whl
Collecting tiktoken>=0.6.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting lm-format-enforcer==0.11.3
  File was already downloaded /home/nexus/amd-ai/wheels/cache/lm_format_enforcer-0.11.3-py3-none-any.whl
Collecting diskcache==5.6.3
  File was already downloaded /home/nexus/amd-ai/wheels/cache/diskcache-5.6.3-py3-none-any.whl
Collecting compressed-tensors==0.12.2
  File was already downloaded /home/nexus/amd-ai/wheels/cache/compressed_tensors-0.12.2-py3-none-any.whl
Collecting depyf==0.20.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/depyf-0.20.0-py3-none-any.whl
Collecting gguf>=0.17.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/gguf-0.17.1-py3-none-any.whl
Collecting mistral_common>=1.8.5 (from mistral_common[image]>=1.8.5)
  File was already downloaded /home/nexus/amd-ai/wheels/cache/mistral_common-1.8.8-py3-none-any.whl
Collecting opencv-python-headless>=4.11.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting einops
  File was already downloaded /home/nexus/amd-ai/wheels/cache/einops-0.8.1-py3-none-any.whl
Collecting numba==0.61.2
  File was already downloaded /home/nexus/amd-ai/wheels/cache/numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0)
  File was already downloaded /home/nexus/amd-ai/wheels/cache/ray-2.53.0-cp311-cp311-manylinux2014_x86_64.whl
Collecting peft
  File was already downloaded /home/nexus/amd-ai/wheels/cache/peft-0.18.0-py3-none-any.whl
Collecting tensorizer==2.10.1
  File was already downloaded /home/nexus/amd-ai/wheels/cache/tensorizer-2.10.1-py3-none-any.whl
Collecting timm>=1.0.17
  File was already downloaded /home/nexus/amd-ai/wheels/cache/timm-1.0.22-py3-none-any.whl
Collecting regex
  File was already downloaded /home/nexus/amd-ai/wheels/cache/regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting cachetools
  File was already downloaded /home/nexus/amd-ai/wheels/cache/cachetools-6.2.4-py3-none-any.whl
Collecting psutil
  File was already downloaded /home/nexus/amd-ai/wheels/cache/psutil-7.2.1-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl
Collecting requests>=2.26.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/requests-2.32.5-py3-none-any.whl
Collecting tqdm
  File was already downloaded /home/nexus/amd-ai/wheels/cache/tqdm-4.67.1-py3-none-any.whl
Collecting blake3
  File was already downloaded /home/nexus/amd-ai/wheels/cache/blake3-1.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting py-cpuinfo
  File was already downloaded /home/nexus/amd-ai/wheels/cache/py_cpuinfo-9.0.0-py3-none-any.whl
Collecting tokenizers>=0.21.1
  File was already downloaded /home/nexus/amd-ai/wheels/cache/tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting prometheus_client>=0.18.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/prometheus_client-0.23.1-py3-none-any.whl
Collecting pillow
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pillow-12.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting prometheus-fastapi-instrumentator>=7.0.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl
Collecting llguidance<1.4.0,>=1.3.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting outlines_core==0.2.11
  File was already downloaded /home/nexus/amd-ai/wheels/cache/outlines_core-0.2.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting lark==1.2.2
  File was already downloaded /home/nexus/amd-ai/wheels/cache/lark-1.2.2-py3-none-any.whl
Collecting xgrammar==0.1.27
  File was already downloaded /home/nexus/amd-ai/wheels/cache/xgrammar-0.1.27-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting typing_extensions>=4.10
  File was already downloaded /home/nexus/amd-ai/wheels/cache/typing_extensions-4.15.0-py3-none-any.whl
Collecting filelock>=3.16.1
  File was already downloaded /home/nexus/amd-ai/wheels/cache/filelock-3.20.1-py3-none-any.whl
Collecting partial-json-parser
  File was already downloaded /home/nexus/amd-ai/wheels/cache/partial_json_parser-0.2.1.1.post7-py3-none-any.whl
Collecting pyzmq>=25.0.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pyzmq-26.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting msgspec
  File was already downloaded /home/nexus/amd-ai/wheels/cache/msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting cloudpickle
  File was already downloaded /home/nexus/amd-ai/wheels/cache/cloudpickle-3.1.2-py3-none-any.whl
Collecting watchfiles
  File was already downloaded /home/nexus/amd-ai/wheels/cache/watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting python-json-logger
  File was already downloaded /home/nexus/amd-ai/wheels/cache/python_json_logger-4.0.0-py3-none-any.whl
Collecting scipy
  File was already downloaded /home/nexus/amd-ai/wheels/cache/scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl
Collecting pybase64
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl
Collecting cbor2
  File was already downloaded /home/nexus/amd-ai/wheels/cache/cbor2-5.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting setproctitle
  File was already downloaded /home/nexus/amd-ai/wheels/cache/setproctitle-1.3.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl
Collecting openai-harmony>=0.0.3
  File was already downloaded /home/nexus/amd-ai/wheels/cache/openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting anthropic==0.71.0
  File was already downloaded /home/nexus/amd-ai/wheels/cache/anthropic-0.71.0-py3-none-any.whl
Collecting model-hosting-container-standards<1.0.0,>=0.1.9
  File was already downloaded /home/nexus/amd-ai/wheels/cache/model_hosting_container_standards-0.1.12-py3-none-any.whl
Collecting datasets
  File was already downloaded /home/nexus/amd-ai/wheels/cache/datasets-4.4.2-py3-none-any.whl
Collecting pytest-asyncio
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pytest_asyncio-1.3.0-py3-none-any.whl
Collecting runai-model-streamer==0.15.0 (from runai-model-streamer[gcs,s3]==0.15.0)
  File was already downloaded /home/nexus/amd-ai/wheels/cache/runai_model_streamer-0.15.0-py3-none-manylinux2014_x86_64.whl
Collecting conch-triton-kernels==1.2.1
  File was already downloaded /home/nexus/amd-ai/wheels/cache/conch_triton_kernels-1.2.1-py3-none-any.whl
Collecting pyyaml
  File was already downloaded /home/nexus/amd-ai/wheels/cache/pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl
Collecting sympy>=1.13.3
  File was already downloaded /home/nexus/amd-ai/wheels/cache/sympy-1.14.0-py3-none-any.whl
Collecting mpmath
  File was already downloaded /home/nexus/amd-ai/wheels/cache/mpmath-1.3.0-py3-none-any.whl
Successfully downloaded lm-format-enforcer diskcache compressed-tensors depyf numba tensorizer outlines_core lark xgrammar anthropic runai-model-streamer conch-triton-kernels pip setuptools wheel ninja cmake packaging pybind11 swig meson meson-python cython transformers accelerate setuptools-scm sentencepiece protobuf fastapi aiohttp openai pydantic tiktoken gguf mistral_common opencv-python-headless einops ray peft timm regex cachetools psutil requests tqdm blake3 py-cpuinfo tokenizers prometheus_client pillow prometheus-fastapi-instrumentator llguidance typing_extensions filelock partial-json-parser pyzmq msgspec cloudpickle watchfiles python-json-logger scipy pybase64 cbor2 setproctitle openai-harmony model-hosting-container-standards datasets pytest-asyncio pyyaml sympy mpmath
>>> Fetching git-based Python dependencies...
>>> Prefetching Triton C++ dependencies...
=== Prefetch Stage Complete ===
Applying Triton ROCm patches...
>>> Internal Build: Max Jobs=25, Mode=pin
>>> Cleaning stale CMake caches from source directories...
>>> CMake cache cleanup complete
ðŸ Setting up Python 3.11 virtual environment...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Requirement already satisfied: pip in /usr/lib/python3/dist-packages (24.0)
Processing ./wheels/cache/pip-25.3-py3-none-any.whl
Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (68.1.2)
Processing ./wheels/cache/setuptools-80.9.0-py3-none-any.whl
Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (0.42.0)
Processing ./wheels/cache/wheel-0.45.1-py3-none-any.whl
Installing collected packages: wheel, setuptools, pip
  WARNING: The script wheel is installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The scripts pip, pip3 and pip3.11 are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed pip-25.3 setuptools-80.9.0 wheel-0.45.1
âœ… Python environment ready
   To activate: source .venv/bin/activate
   Or use: ./activate_env.sh

ðŸ“‹ Installed packages:
Package             Version
------------------- ---------------
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
âœ… NumPy already exists in artifacts/, skipping build.
>>> Installing existing artifact: torch-*.whl
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/torch-2.9.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: torch
  WARNING: The scripts torchfrtrace and torchrun are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed torch-2.9.1
ðŸ—ï¸  Building PyTorch 2.9.1 with ROCm support...
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
âœ… PyTorch already exists in artifacts/, skipping build.
>>> Verifying PyTorch 2.9.1 artifact exists...
-rw-rw-r-- 1 ubuntu ubuntu 523M Dec 29 15:14 /app/artifacts/torch-2.9.1-cp311-cp311-linux_x86_64.whl
>>> Installing existing artifact: triton-*.whl
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/triton-3.1.0-cp311-cp311-linux_x86_64.whl
Installing collected packages: triton
Successfully installed triton-3.1.0
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
âœ… Triton already exists in artifacts/, skipping build.
>>> Installing existing artifact: pillow-*.whl
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/pillow-10.4.0-cp311-cp311-linux_x86_64.whl
Installing collected packages: pillow
Successfully installed pillow-10.4.0
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
============================================
Building Pillow-SIMD 10.4.0 (AVX-512)
============================================
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
Building for Architecture: znver5
Looking in links: /app/artifacts, /app/wheels/cache
Processing /app/src/extras/pillow-simd
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  /tmp/.local/lib/python3.11/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated
  !!

          ********************************************************************************
          Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).

          By 2026-Feb-18, you need to update your project and remove deprecated calls
          or your builds will no longer be supported.

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    corresp(dist, value, root_dir)
  /tmp/.local/lib/python3.11/site-packages/setuptools/config/_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: Historical Permission Notice and Disclaimer (HPND)

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    dist._finalize_license_expression()
  /tmp/.local/lib/python3.11/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: Historical Permission Notice and Disclaimer (HPND)

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    self._finalize_license_expression()
  running dist_info
  creating /tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info
  writing /tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/PKG-INFO
  writing dependency_links to /tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/dependency_links.txt
  writing requirements to /tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/requires.txt
  writing top-level names to /tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/top_level.txt
  writing manifest file '/tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/SOURCES.txt'
  reading manifest file '/tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no files found matching '*.c'
  warning: no files found matching '*.h'
  warning: no files found matching '*.sh'
  warning: no files found matching '*.txt'
  warning: no files found matching '.flake8'
  warning: no previously-included files found matching '.appveyor.yml'
  warning: no previously-included files found matching '.clang-format'
  warning: no previously-included files found matching '.coveragerc'
  warning: no previously-included files found matching '.editorconfig'
  warning: no previously-included files found matching '.readthedocs.yml'
  warning: no previously-included files found matching 'codecov.yml'
  warning: no previously-included files found matching 'renovate.json'
  warning: no previously-included files matching '.git*' found anywhere in distribution
  warning: no previously-included files matching '*.so' found anywhere in distribution
  no previously-included directories found matching '.ci'
  no previously-included directories found matching 'wheels'
  adding license file 'LICENSE'
  writing manifest file '/tmp/pip-modern-metadata-hpy4b3ou/pillow.egg-info/SOURCES.txt'
  creating '/tmp/pip-modern-metadata-hpy4b3ou/pillow-10.4.0.dist-info'
  Preparing metadata (pyproject.toml): finished with status 'done'
Building wheels for collected packages: pillow
  Building wheel for pillow (pyproject.toml): started
  Running command Building wheel for pillow (pyproject.toml)
  /tmp/.local/lib/python3.11/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated
  !!

          ********************************************************************************
          Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).

          By 2026-Feb-18, you need to update your project and remove deprecated calls
          or your builds will no longer be supported.

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    corresp(dist, value, root_dir)
  /tmp/.local/lib/python3.11/site-packages/setuptools/config/_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: Historical Permission Notice and Disclaimer (HPND)

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    dist._finalize_license_expression()
  /tmp/.local/lib/python3.11/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: Historical Permission Notice and Disclaimer (HPND)

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    self._finalize_license_expression()
  running bdist_wheel
  running build
  running build_py
  creating build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GdImageFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/report.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/IcoImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageShow.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/MspImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/BufrStubImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/TiffTags.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/McIdasImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageCms.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PsdImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/features.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/FitsImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageGrab.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/SpiderImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GribStubImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/Hdf5StubImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImagePath.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/FtexImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/WmfImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImtImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/FpxImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/SunImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/Jpeg2KImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageOps.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageColor.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/DdsImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/MicImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PpmImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/BlpImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/CurImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PdfParser.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageWin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/TiffImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_binary.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/DcxImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/XpmImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/TarIO.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageDraw2.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/IptcImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/MpoImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/Image.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PalmImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/XVThumbImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/MpegImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageDraw.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/SgiImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageSequence.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PSDraw.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageMorph.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/FliImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/TgaImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageTk.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageFont.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/EpsImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/BdfFontFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_tkinter_finder.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GimpPaletteFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageFilter.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ExifTags.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_deprecate.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PcxImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/QoiImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/IcnsImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ContainerIO.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GbrImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageMath.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageStat.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/XbmImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageChops.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PdfImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PaletteFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageQt.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PyAccess.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageMode.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GimpGradientFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageEnhance.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/__init__.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/JpegPresets.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PixarImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/__main__.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_util.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_typing.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_version.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/BmpImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PcfFontFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/JpegImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/GifImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PcdImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/PngImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/WebPImagePlugin.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImageTransform.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/FontFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/ImagePalette.py -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/WalImageFile.py -> build/lib.linux-x86_64-cpython-311/PIL
  running egg_info
  writing src/pillow.egg-info/PKG-INFO
  writing dependency_links to src/pillow.egg-info/dependency_links.txt
  writing requirements to src/pillow.egg-info/requires.txt
  writing top-level names to src/pillow.egg-info/top_level.txt
  reading manifest file 'src/pillow.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no files found matching '*.c'
  warning: no files found matching '*.h'
  warning: no files found matching '*.sh'
  warning: no files found matching '*.txt'
  warning: no files found matching '.flake8'
  warning: no previously-included files found matching '.appveyor.yml'
  warning: no previously-included files found matching '.clang-format'
  warning: no previously-included files found matching '.coveragerc'
  warning: no previously-included files found matching '.editorconfig'
  warning: no previously-included files found matching '.readthedocs.yml'
  warning: no previously-included files found matching 'codecov.yml'
  warning: no previously-included files found matching 'renovate.json'
  warning: no previously-included files matching '.git*' found anywhere in distribution
  warning: no previously-included files matching '*.so' found anywhere in distribution
  no previously-included directories found matching '.ci'
  no previously-included directories found matching 'wheels'
  adding license file 'LICENSE'
  writing manifest file 'src/pillow.egg-info/SOURCES.txt'
  copying src/PIL/_imaging.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_imagingcms.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_imagingft.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_imagingmath.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_imagingmorph.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/_webp.pyi -> build/lib.linux-x86_64-cpython-311/PIL
  copying src/PIL/py.typed -> build/lib.linux-x86_64-cpython-311/PIL
  running build_ext
  building 'PIL._imaging' extension
  creating build/temp.linux-x86_64-cpython-311/src
  building 'PIL._webp' extension
  building 'PIL._imagingft' extension
  creating build/temp.linux-x86_64-cpython-311/src
  creating build/temp.linux-x86_64-cpython-311/src
  building 'PIL._imagingcms' extension
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_WEBPMUX -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_webp.c -o build/temp.linux-x86_64-cpython-311/src/_webp.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imagingft.c -o build/temp.linux-x86_64-cpython-311/src/_imagingft.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imagingcms.c -o build/temp.linux-x86_64-cpython-311/src/_imagingcms.o
  creating build/temp.linux-x86_64-cpython-311/src/libImaging
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imaging.c -o build/temp.linux-x86_64-cpython-311/src/_imaging.o
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_webp.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -lwebp -lwebpmux -lwebpdemux -o build/lib.linux-x86_64-cpython-311/PIL/_webp.cpython-311-x86_64-linux-gnu.so
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_imagingft.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -lfreetype -o build/lib.linux-x86_64-cpython-311/PIL/_imagingft.cpython-311-x86_64-linux-gnu.so
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_imagingcms.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -llcms2 -o build/lib.linux-x86_64-cpython-311/PIL/_imagingcms.cpython-311-x86_64-linux-gnu.so
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/decode.c -o build/temp.linux-x86_64-cpython-311/src/decode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/display.c -o build/temp.linux-x86_64-cpython-311/src/display.o
  building 'PIL._imagingtk' extension
  creating build/temp.linux-x86_64-cpython-311/src/Tk
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/Tk/tkImaging.c -o build/temp.linux-x86_64-cpython-311/src/Tk/tkImaging.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/encode.c -o build/temp.linux-x86_64-cpython-311/src/encode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imagingtk.c -o build/temp.linux-x86_64-cpython-311/src/_imagingtk.o
  building 'PIL._imagingmath' extension
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imagingmath.c -o build/temp.linux-x86_64-cpython-311/src/_imagingmath.o
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/Tk/tkImaging.o build/temp.linux-x86_64-cpython-311/src/_imagingtk.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-311/PIL/_imagingtk.cpython-311-x86_64-linux-gnu.so
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Access.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Access.o
  building 'PIL._imagingmorph' extension
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/_imagingmorph.c -o build/temp.linux-x86_64-cpython-311/src/_imagingmorph.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/AlphaComposite.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/AlphaComposite.o
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_imagingmath.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-311/PIL/_imagingmath.cpython-311-x86_64-linux-gnu.so
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_imagingmorph.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-311/PIL/_imagingmorph.cpython-311-x86_64-linux-gnu.so
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Bands.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Bands.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/BcnDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/BcnDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/BitDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/BitDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Blend.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Blend.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/BoxBlur.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/BoxBlur.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Chops.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Chops.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/ColorLUT.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/ColorLUT.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Convert.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Convert.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/ConvertYCbCr.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/ConvertYCbCr.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Copy.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Copy.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Crop.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Crop.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Dib.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Dib.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Draw.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Draw.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Effects.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Effects.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/EpsEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/EpsEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/File.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/File.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Fill.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Fill.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Filter.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Filter.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/FliDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/FliDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Geometry.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Geometry.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/GetBBox.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/GetBBox.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/GifDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/GifDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/GifEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/GifEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/HexDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/HexDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Histo.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Histo.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Jpeg2KDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Jpeg2KDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Jpeg2KEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Jpeg2KEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/JpegDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/JpegDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/JpegEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/JpegEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Matrix.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Matrix.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/ModeFilter.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/ModeFilter.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Negative.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Negative.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Offset.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Offset.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Pack.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Pack.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/PackDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/PackDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Palette.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Palette.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Paste.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Paste.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/PcdDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/PcdDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/PcxDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/PcxDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/PcxEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/PcxEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Point.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Point.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Quant.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Quant.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/QuantHash.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantHash.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/QuantHeap.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantHeap.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/QuantOctree.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantOctree.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/QuantPngQuant.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantPngQuant.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/RankFilter.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/RankFilter.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/RawDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/RawDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/RawEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/RawEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Reduce.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Reduce.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Resample.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Resample.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/SgiRleDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/SgiRleDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Storage.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Storage.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/SunRleDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/SunRleDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/TgaRleDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/TgaRleDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/TgaRleEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/TgaRleEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/TiffDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/TiffDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/Unpack.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/Unpack.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/UnpackYCC.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/UnpackYCC.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/UnsharpMask.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/UnsharpMask.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/XbmDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/XbmDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/XbmEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/XbmEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/ZipDecode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/ZipDecode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/ZipEncode.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/ZipEncode.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/libImaging/codec_fd.c -o build/temp.linux-x86_64-cpython-311/src/libImaging/codec_fd.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/map.c -o build/temp.linux-x86_64-cpython-311/src/map.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/outline.c -o build/temp.linux-x86_64-cpython-311/src/outline.o
  gcc -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto -fPIC -DHAVE_LIBTIFF -DHAVE_LIBJPEG -DHAVE_LIBZ -DHAVE_XCB -DPILLOW_VERSION=\"10.4.0\" -I/usr/include/freetype2 -I/usr/include -I/usr/include/x86_64-linux-gnu -I/usr/include/webp -I/usr/include/libpng16 -I/usr/local/include -I/usr/include/python3.11 -c src/path.c -o build/temp.linux-x86_64-cpython-311/src/path.o
  gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec -lm -lmvec -O3 -march=znver5 -mtune=znver5 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vbmi -flto=auto build/temp.linux-x86_64-cpython-311/src/_imaging.o build/temp.linux-x86_64-cpython-311/src/decode.o build/temp.linux-x86_64-cpython-311/src/display.o build/temp.linux-x86_64-cpython-311/src/encode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Access.o build/temp.linux-x86_64-cpython-311/src/libImaging/AlphaComposite.o build/temp.linux-x86_64-cpython-311/src/libImaging/Bands.o build/temp.linux-x86_64-cpython-311/src/libImaging/BcnDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/BitDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Blend.o build/temp.linux-x86_64-cpython-311/src/libImaging/BoxBlur.o build/temp.linux-x86_64-cpython-311/src/libImaging/Chops.o build/temp.linux-x86_64-cpython-311/src/libImaging/ColorLUT.o build/temp.linux-x86_64-cpython-311/src/libImaging/Convert.o build/temp.linux-x86_64-cpython-311/src/libImaging/ConvertYCbCr.o build/temp.linux-x86_64-cpython-311/src/libImaging/Copy.o build/temp.linux-x86_64-cpython-311/src/libImaging/Crop.o build/temp.linux-x86_64-cpython-311/src/libImaging/Dib.o build/temp.linux-x86_64-cpython-311/src/libImaging/Draw.o build/temp.linux-x86_64-cpython-311/src/libImaging/Effects.o build/temp.linux-x86_64-cpython-311/src/libImaging/EpsEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/File.o build/temp.linux-x86_64-cpython-311/src/libImaging/Fill.o build/temp.linux-x86_64-cpython-311/src/libImaging/Filter.o build/temp.linux-x86_64-cpython-311/src/libImaging/FliDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Geometry.o build/temp.linux-x86_64-cpython-311/src/libImaging/GetBBox.o build/temp.linux-x86_64-cpython-311/src/libImaging/GifDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/GifEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/HexDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Histo.o build/temp.linux-x86_64-cpython-311/src/libImaging/Jpeg2KDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Jpeg2KEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/JpegDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/JpegEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Matrix.o build/temp.linux-x86_64-cpython-311/src/libImaging/ModeFilter.o build/temp.linux-x86_64-cpython-311/src/libImaging/Negative.o build/temp.linux-x86_64-cpython-311/src/libImaging/Offset.o build/temp.linux-x86_64-cpython-311/src/libImaging/Pack.o build/temp.linux-x86_64-cpython-311/src/libImaging/PackDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Palette.o build/temp.linux-x86_64-cpython-311/src/libImaging/Paste.o build/temp.linux-x86_64-cpython-311/src/libImaging/PcdDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/PcxDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/PcxEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Point.o build/temp.linux-x86_64-cpython-311/src/libImaging/Quant.o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantHash.o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantHeap.o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantOctree.o build/temp.linux-x86_64-cpython-311/src/libImaging/QuantPngQuant.o build/temp.linux-x86_64-cpython-311/src/libImaging/RankFilter.o build/temp.linux-x86_64-cpython-311/src/libImaging/RawDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/RawEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Reduce.o build/temp.linux-x86_64-cpython-311/src/libImaging/Resample.o build/temp.linux-x86_64-cpython-311/src/libImaging/SgiRleDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Storage.o build/temp.linux-x86_64-cpython-311/src/libImaging/SunRleDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/TgaRleDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/TgaRleEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/TiffDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/Unpack.o build/temp.linux-x86_64-cpython-311/src/libImaging/UnpackYCC.o build/temp.linux-x86_64-cpython-311/src/libImaging/UnsharpMask.o build/temp.linux-x86_64-cpython-311/src/libImaging/XbmDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/XbmEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/ZipDecode.o build/temp.linux-x86_64-cpython-311/src/libImaging/ZipEncode.o build/temp.linux-x86_64-cpython-311/src/libImaging/codec_fd.o build/temp.linux-x86_64-cpython-311/src/map.o build/temp.linux-x86_64-cpython-311/src/outline.o build/temp.linux-x86_64-cpython-311/src/path.o -L/usr/lib/x86_64-linux-gnu -L/usr/lib -L/opt/rocm-7.1.1/lib -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu -ltiff -ljpeg -lz -lxcb -o build/lib.linux-x86_64-cpython-311/PIL/_imaging.cpython-311-x86_64-linux-gnu.so
  --------------------------------------------------------------------
  PIL SETUP SUMMARY
  --------------------------------------------------------------------
  version      Pillow 10.4.0
  platform     linux 3.11.14 (main, Oct 10 2025, 08:54:04)
               [GCC 13.3.0]
  --------------------------------------------------------------------
  --- JPEG support available
  *** OPENJPEG (JPEG2000) support not available
  --- ZLIB (PNG/ZIP) support available
  *** LIBIMAGEQUANT support not available
  --- LIBTIFF support available
  --- FREETYPE2 support available
  *** RAQM (Text shaping) support not available
  --- LITTLECMS2 support available
  --- WEBP support available
  --- WEBPMUX support available
  --- XCB (X protocol) support available
  --------------------------------------------------------------------
  To add a missing option, make sure you have the required
  library and headers.
  See https://pillow.readthedocs.io/en/latest/installation.html#building-from-source

  To check the build, run the selftest.py script.

  installing to build/bdist.linux-x86_64/wheel
  running install
  running install_lib
  creating build/bdist.linux-x86_64/wheel
  creating build/bdist.linux-x86_64/wheel/PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GdImageFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/report.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/IcoImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingft.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingft.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageShow.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/MspImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/BufrStubImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingmorph.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/TiffTags.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/McIdasImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageCms.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PsdImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/features.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingmath.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/FitsImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageGrab.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/SpiderImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GribStubImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/Hdf5StubImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImagePath.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/FtexImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/WmfImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImtImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/FpxImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/SunImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingtk.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/Jpeg2KImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageOps.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageColor.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_webp.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/DdsImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/MicImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PpmImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/BlpImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/CurImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PdfParser.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageWin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/TiffImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_binary.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/DcxImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/XpmImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/TarIO.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingcms.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageDraw2.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/IptcImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imaging.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_webp.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/MpoImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/Image.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PalmImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/XVThumbImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/MpegImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageDraw.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/SgiImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageSequence.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PSDraw.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageMorph.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/FliImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/TgaImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageTk.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageFont.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/EpsImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/BdfFontFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_tkinter_finder.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GimpPaletteFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageFilter.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ExifTags.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_deprecate.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PcxImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/QoiImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/IcnsImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ContainerIO.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GbrImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageMath.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/py.typed -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageStat.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingcms.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/XbmImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageChops.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PdfImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PaletteFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageQt.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PyAccess.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageMode.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GimpGradientFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageEnhance.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/__init__.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/JpegPresets.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PixarImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingmath.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/__main__.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_util.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_typing.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_version.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/BmpImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PcfFontFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imagingmorph.pyi -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/JpegImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/GifImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/_imaging.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PcdImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/PngImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/WebPImagePlugin.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImageTransform.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/FontFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/ImagePalette.py -> build/bdist.linux-x86_64/wheel/./PIL
  copying build/lib.linux-x86_64-cpython-311/PIL/WalImageFile.py -> build/bdist.linux-x86_64/wheel/./PIL
  running install_egg_info
  Copying src/pillow.egg-info to build/bdist.linux-x86_64/wheel/./pillow-10.4.0-py3.11.egg-info
  running install_scripts
  creating build/bdist.linux-x86_64/wheel/pillow-10.4.0.dist-info/WHEEL
  creating '/tmp/.cache/pip/wheels/0b/bd/58/17b88d092e0e4630038c45ea8eac5dfb5e5d8ab85b097b9e3c/tmpq1q5xbe3/.tmp-w4z3dsk0/pillow-10.4.0-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
  adding 'PIL/BdfFontFile.py'
  adding 'PIL/BlpImagePlugin.py'
  adding 'PIL/BmpImagePlugin.py'
  adding 'PIL/BufrStubImagePlugin.py'
  adding 'PIL/ContainerIO.py'
  adding 'PIL/CurImagePlugin.py'
  adding 'PIL/DcxImagePlugin.py'
  adding 'PIL/DdsImagePlugin.py'
  adding 'PIL/EpsImagePlugin.py'
  adding 'PIL/ExifTags.py'
  adding 'PIL/FitsImagePlugin.py'
  adding 'PIL/FliImagePlugin.py'
  adding 'PIL/FontFile.py'
  adding 'PIL/FpxImagePlugin.py'
  adding 'PIL/FtexImagePlugin.py'
  adding 'PIL/GbrImagePlugin.py'
  adding 'PIL/GdImageFile.py'
  adding 'PIL/GifImagePlugin.py'
  adding 'PIL/GimpGradientFile.py'
  adding 'PIL/GimpPaletteFile.py'
  adding 'PIL/GribStubImagePlugin.py'
  adding 'PIL/Hdf5StubImagePlugin.py'
  adding 'PIL/IcnsImagePlugin.py'
  adding 'PIL/IcoImagePlugin.py'
  adding 'PIL/ImImagePlugin.py'
  adding 'PIL/Image.py'
  adding 'PIL/ImageChops.py'
  adding 'PIL/ImageCms.py'
  adding 'PIL/ImageColor.py'
  adding 'PIL/ImageDraw.py'
  adding 'PIL/ImageDraw2.py'
  adding 'PIL/ImageEnhance.py'
  adding 'PIL/ImageFile.py'
  adding 'PIL/ImageFilter.py'
  adding 'PIL/ImageFont.py'
  adding 'PIL/ImageGrab.py'
  adding 'PIL/ImageMath.py'
  adding 'PIL/ImageMode.py'
  adding 'PIL/ImageMorph.py'
  adding 'PIL/ImageOps.py'
  adding 'PIL/ImagePalette.py'
  adding 'PIL/ImagePath.py'
  adding 'PIL/ImageQt.py'
  adding 'PIL/ImageSequence.py'
  adding 'PIL/ImageShow.py'
  adding 'PIL/ImageStat.py'
  adding 'PIL/ImageTk.py'
  adding 'PIL/ImageTransform.py'
  adding 'PIL/ImageWin.py'
  adding 'PIL/ImtImagePlugin.py'
  adding 'PIL/IptcImagePlugin.py'
  adding 'PIL/Jpeg2KImagePlugin.py'
  adding 'PIL/JpegImagePlugin.py'
  adding 'PIL/JpegPresets.py'
  adding 'PIL/McIdasImagePlugin.py'
  adding 'PIL/MicImagePlugin.py'
  adding 'PIL/MpegImagePlugin.py'
  adding 'PIL/MpoImagePlugin.py'
  adding 'PIL/MspImagePlugin.py'
  adding 'PIL/PSDraw.py'
  adding 'PIL/PaletteFile.py'
  adding 'PIL/PalmImagePlugin.py'
  adding 'PIL/PcdImagePlugin.py'
  adding 'PIL/PcfFontFile.py'
  adding 'PIL/PcxImagePlugin.py'
  adding 'PIL/PdfImagePlugin.py'
  adding 'PIL/PdfParser.py'
  adding 'PIL/PixarImagePlugin.py'
  adding 'PIL/PngImagePlugin.py'
  adding 'PIL/PpmImagePlugin.py'
  adding 'PIL/PsdImagePlugin.py'
  adding 'PIL/PyAccess.py'
  adding 'PIL/QoiImagePlugin.py'
  adding 'PIL/SgiImagePlugin.py'
  adding 'PIL/SpiderImagePlugin.py'
  adding 'PIL/SunImagePlugin.py'
  adding 'PIL/TarIO.py'
  adding 'PIL/TgaImagePlugin.py'
  adding 'PIL/TiffImagePlugin.py'
  adding 'PIL/TiffTags.py'
  adding 'PIL/WalImageFile.py'
  adding 'PIL/WebPImagePlugin.py'
  adding 'PIL/WmfImagePlugin.py'
  adding 'PIL/XVThumbImagePlugin.py'
  adding 'PIL/XbmImagePlugin.py'
  adding 'PIL/XpmImagePlugin.py'
  adding 'PIL/__init__.py'
  adding 'PIL/__main__.py'
  adding 'PIL/_binary.py'
  adding 'PIL/_deprecate.py'
  adding 'PIL/_imaging.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_imaging.pyi'
  adding 'PIL/_imagingcms.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_imagingcms.pyi'
  adding 'PIL/_imagingft.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_imagingft.pyi'
  adding 'PIL/_imagingmath.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_imagingmath.pyi'
  adding 'PIL/_imagingmorph.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_imagingmorph.pyi'
  adding 'PIL/_imagingtk.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_tkinter_finder.py'
  adding 'PIL/_typing.py'
  adding 'PIL/_util.py'
  adding 'PIL/_version.py'
  adding 'PIL/_webp.cpython-311-x86_64-linux-gnu.so'
  adding 'PIL/_webp.pyi'
  adding 'PIL/features.py'
  adding 'PIL/py.typed'
  adding 'PIL/report.py'
  adding 'pillow-10.4.0.dist-info/licenses/LICENSE'
  adding 'pillow-10.4.0.dist-info/METADATA'
  adding 'pillow-10.4.0.dist-info/WHEEL'
  adding 'pillow-10.4.0.dist-info/top_level.txt'
  adding 'pillow-10.4.0.dist-info/zip-safe'
  adding 'pillow-10.4.0.dist-info/RECORD'
  removing build/bdist.linux-x86_64/wheel
  Building wheel for pillow (pyproject.toml): finished with status 'done'
  Created wheel for pillow: filename=pillow-10.4.0-cp311-cp311-linux_x86_64.whl size=1006491 sha256=06c86d02d64af4dcdf7f3518df58e4cd9cdc1bb1f94298fb3343c3dde201951b
  Stored in directory: /tmp/.cache/pip/wheels/0b/bd/58/17b88d092e0e4630038c45ea8eac5dfb5e5d8ab85b097b9e3c
Successfully built pillow
Found existing installation: pillow 10.4.0
Uninstalling pillow-10.4.0:
  Successfully uninstalled pillow-10.4.0
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing /app/artifacts/pillow-10.4.0-cp311-cp311-linux_x86_64.whl
Installing collected packages: pillow
Successfully installed pillow-10.4.0

=== Verification ===
Pillow version: 10.4.0
SIMD support: False

=== Pillow-SIMD build complete ===
Wheel: /app/artifacts/pillow*.whl
>>> Installing existing artifact: torchvision-*.whl
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/torchvision-0.20.1a0+3ac97aa-cp311-cp311-linux_x86_64.whl
Installing collected packages: torchvision
Successfully installed torchvision-0.20.1a0+3ac97aa
>>> Installing existing artifact: torchaudio-*.whl
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/torchaudio-2.5.1a0+1661daf-cp311-cp311-linux_x86_64.whl
Installing collected packages: torchaudio
Successfully installed torchaudio-2.5.1a0+1661daf
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
============================================
Checking TorchVision 0.20.1
============================================
âœ… TorchVision already exists, skipping build.
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/torchvision-0.20.1a0+3ac97aa-cp311-cp311-linux_x86_64.whl
Installing collected packages: torchvision
  Attempting uninstall: torchvision
    Found existing installation: torchvision 0.20.1a0+3ac97aa
    Uninstalling torchvision-0.20.1a0+3ac97aa:
      Successfully uninstalled torchvision-0.20.1a0+3ac97aa
Successfully installed torchvision-0.20.1a0+3ac97aa
============================================
Checking TorchAudio 2.5.1
============================================
âœ… TorchAudio already exists, skipping build.
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/torchaudio-2.5.1a0+1661daf-cp311-cp311-linux_x86_64.whl
Installing collected packages: torchaudio
  Attempting uninstall: torchaudio
    Found existing installation: torchaudio 2.5.1a0+1661daf
    Uninstalling torchaudio-2.5.1a0+1661daf:
      Successfully uninstalled torchaudio-2.5.1a0+1661daf
Successfully installed torchaudio-2.5.1a0+1661daf
torch: 2.9.1
torchvision: 0.20.1a0+3ac97aa
torchaudio: 2.5.1a0+1661daf
âœ… TorchVision/TorchAudio build complete
âš ï¸  Script already locked: ./scripts/23_build_torchvision_audio.sh
{
    "locked_at": "2025-12-29_18-12-28",
    "version": "1df0b9e",
    "script": "./scripts/23_build_torchvision_audio.sh",
    "artifact": "torchvision+torchaudio",
    "backup": "/app/backups/locks/23_build_torchvision_audio_2025-12-29_18-12-28_1df0b9e",
    "locked_by": "lock_manager"
}
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
âœ… Flash Attention already exists in artifacts/, skipping build.
ðŸ“¦ Installing NumPy from artifacts...
Defaulting to user installation because normal site-packages is not writeable
Looking in links: /app/artifacts, /app/wheels/cache
Processing ./artifacts/numpy-2.2.1-cp311-cp311-linux_x86_64.whl
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.1
    Uninstalling numpy-2.2.1:
      Successfully uninstalled numpy-2.2.1
  WARNING: The scripts f2py and numpy-config are installed in '/tmp/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed numpy-2.2.1
ðŸš€ Setting up ROCm environment...
âœ… ROCm environment set for gfx1151
   ROCM_PATH: /opt/rocm-7.1.1
   HIP_PATH: /opt/rocm-7.1.1
   GPU Target: gfx1151
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
âš¡ Setting up CPU-optimized environment...
âœ… CPU environment optimized for znver5
   CFLAGS: -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto
   AVX-512 extensions: F, BW, VL, DQ, CD, VBMI, VBMI2, VNNI, BITALG, VPOPCNTDQ, BF16
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
============================================
Building xFormers 0.0.29 for ROCm
============================================
ðŸ§® Parallel config -> jobs=25
    Mode Config: PARALLEL_MODE=pin, PIN=none
    System: Usage Cpus=25, Host Cpus=32, Mem=30GiB (Standard mode)
    MAKEFLAGS= -j25
    NINJAFLAGS= -j25
    CMAKE_BUILD_PARALLEL_LEVEL=25
    CCACHE_MAXSIZE=20G
    LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -lm -lmvec
Looking in links: /app/artifacts, /app/wheels/cache, /app/artifacts, /app/wheels/cache
Processing /app/src/extras/xformers
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  /app/src/extras/xformers/xformers/csrc/attention/attention.cpp -> /app/src/extras/xformers/xformers/csrc/attention/attention.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/macros.h -> /app/src/extras/xformers/xformers/csrc/attention/macros.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/matmul.h -> /app/src/extras/xformers/xformers/csrc/attention/matmul.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/autograd/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/autograd/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/sddmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/sddmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/sparse_softmax.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/sparse_softmax.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/spmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/spmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_util.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_util.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_params.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_params.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.hip [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integer.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integral_constant.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integral_constant.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/type_traits.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/type_traits.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/bit_cast.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/bit_cast.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/math.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/math.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/to_sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/to_sequence_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/sequence_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/functional.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/functional_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/array_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/tuple.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/tuple_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/multi_index.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/multi_index_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/map.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/map_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/container_helper.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/container_helper_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/magic_div.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/magic_div_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/coordinate_transform.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/coordinate_transform_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/numeric.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/numeric.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/cluster_descriptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/cluster_descriptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/statically_indexed_array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/statically_indexed_array_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/space_filling_curve.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/space_filling_curve_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/half.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/half.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/random.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/random.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/float8.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/float8.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/bfloat16.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/bfloat16.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/vector_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/vector_type_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/thread_buffer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/thread_buffer_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/type_convert.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/type_convert.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/generic_memory_space_atomic.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/generic_memory_space_atomic_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/utility.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/utility.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/meta_data_buffer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/meta_data_buffer_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/span.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/span.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/null_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/null_type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/buffer_view.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/buffer_view_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_coordinate.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_coordinate_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_encoding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_encoding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/static_distributed_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/static_distributed_tensor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_window.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_window_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_descriptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_descriptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_view.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_view_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tile_window.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tile_window_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tensor.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/load_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/load_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_elementwise.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_elementwise_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/transpose_vectors.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/transpose_vectors_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/shuffle_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/shuffle_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/slice_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/slice_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/store_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/store_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/sweep_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/sweep_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_coordinate.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_coordinate_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/update_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/update_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/ignore.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/ignore.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/philox_rand.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/philox_rand.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/unary_element_function.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/unary_element_function_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_impl.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_impl_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_dropout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_dropout_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_masking.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_masking_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_position_encoding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_position_encoding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/page_block_navigator.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/page_block_navigator_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common/tensor_layout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common/tensor_layout.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp [ok]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/reduce/block/block_reduce.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/reduce/block/block_reduce_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_rand_uniform_kernel.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_rand_uniform_kernel_hip.h [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/stream_config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/stream_config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/hip_check_error.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/hip_check_error.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/timer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/timer.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/env.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/env.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/ck.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/ck.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/integral_constant.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/integral_constant.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/enable_if.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/enable_if.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/number.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/number.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/math.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/math.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/sequence.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional2.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional2.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/tuple.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/tuple.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/statically_indexed_array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/statically_indexed_array.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/data_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/data_type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/f8_utils.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/f8_utils.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/random_gen.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/random_gen.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/array.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type_convert.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type_convert.hpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_inner_product.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_inner_product.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_math_ext.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_math_ext.h [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/stream_config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/stream_config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/hip_check_error.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/hip_check_error.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/kernel_launch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/kernel_launch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/tensor_operation/gpu/device/device_base.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/tensor_operation/gpu/device/device_base.hpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_decoder.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_decoder.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_splitk.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_splitk_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_splitk.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_splitk.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_test.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_test.cu [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_bool_switch.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_bool_switch.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_bwd_setting.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_bwd_setting_hip.h [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/arg_parser.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/arg_parser.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/ranges.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/ranges.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/device_memory.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/device_memory_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/fill.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/fill_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/host_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/host_tensor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_dropout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_dropout_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_elementwise.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_elementwise_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_masking.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_masking_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_rotary_position_embedding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_rotary_position_embedding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_softmax.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_softmax_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_im2col.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_im2col_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_layernorm2d.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_layernorm2d_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_reduce.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_reduce_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_softmax.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_softmax_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue/default_2d_epilogue.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue/default_2d_epilogue_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_headdim_switch.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_headdim_switch_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_fwd_setting.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_fwd_setting_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/sddmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/sddmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/sparse_softmax.cpp -> /app/src/extras/xformers/xformers/csrc/attention/sparse_softmax.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/spmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/spmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/memset_32b.cpp -> /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/memset_32b.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/synchronization.cpp -> /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/synchronization.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sparse24/sparse24.cpp -> /app/src/extras/xformers/xformers/csrc/sparse24/sparse24.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_op.cpp -> /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_op.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_packedw.cpp -> /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_packedw.cpp [skipped, no changes]
  [92mSuccessfully preprocessed all matching files.[0m
  /tmp/.local/lib/python3.11/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: BSD License

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    self._finalize_license_expression()
  Total number of unsupported CUDA function calls: 0


  Total number of replaced kernel launches: 10
  running dist_info
  creating /tmp/pip-modern-metadata-k2v01hos/xformers.egg-info
  writing /tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/PKG-INFO
  writing dependency_links to /tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/dependency_links.txt
  writing requirements to /tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/requires.txt
  writing top-level names to /tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/top_level.txt
  writing manifest file '/tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/SOURCES.txt'
  reading manifest file '/tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no files found matching 'third_party/flash-attention/version.txt'
  adding license file 'LICENSE'
  writing manifest file '/tmp/pip-modern-metadata-k2v01hos/xformers.egg-info/SOURCES.txt'
  creating '/tmp/pip-modern-metadata-k2v01hos/xformers-0.0.30+56be3b5e.d20251229.dist-info'
  Preparing metadata (pyproject.toml): finished with status 'done'
Building wheels for collected packages: xformers
  Building wheel for xformers (pyproject.toml): started
  Running command Building wheel for xformers (pyproject.toml)
  /app/src/extras/xformers/xformers/csrc/attention/attention.cpp -> /app/src/extras/xformers/xformers/csrc/attention/attention.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/macros.h -> /app/src/extras/xformers/xformers/csrc/attention/macros.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/matmul.h -> /app/src/extras/xformers/xformers/csrc/attention/matmul.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/autograd/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/autograd/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/sddmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/sddmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/sparse_softmax.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/sparse_softmax.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/cpu/spmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/cpu/spmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_util.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_util.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_params.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_params.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.hip [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integer.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integral_constant.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/integral_constant.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/type_traits.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/type_traits.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/bit_cast.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/bit_cast.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/math.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/math.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/to_sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/to_sequence_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/sequence_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/functional.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/functional_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/array_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/tuple.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/tuple_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/multi_index.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/multi_index_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/map.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/map_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/container_helper.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/container_helper_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/magic_div.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/magic_div_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/coordinate_transform.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/coordinate_transform_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/numeric.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/numeric.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/cluster_descriptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/cluster_descriptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/statically_indexed_array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/statically_indexed_array_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/space_filling_curve.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/algorithm/space_filling_curve_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/half.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/half.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/random.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/random.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/float8.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/float8.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/bfloat16.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/bfloat16.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/vector_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/vector_type_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/thread_buffer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/thread_buffer_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/type_convert.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/type_convert.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/generic_memory_space_atomic.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/generic_memory_space_atomic_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/utility.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/utility.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/meta_data_buffer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/meta_data_buffer_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/span.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/container/span.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/null_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/numeric/null_type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/buffer_view.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/buffer_view_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_coordinate.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_adaptor_coordinate_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_encoding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_encoding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_distribution_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/static_distributed_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/static_distributed_tensor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_window.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_window_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_descriptor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_descriptor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_view.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_view_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tile_window.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tile_window_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/null_tensor.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/load_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/load_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_elementwise.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tile_elementwise_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/transpose_vectors.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/transpose_vectors_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/shuffle_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/shuffle_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/slice_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/slice_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/store_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/store_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/sweep_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/sweep_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_coordinate.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/tensor_coordinate_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/update_tile.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/tensor/update_tile_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/ignore.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/ignore.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/philox_rand.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/philox_rand.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/unary_element_function.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/utility/unary_element_function_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_impl.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_impl_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_dropout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_dropout_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_masking.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_masking_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_position_encoding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_position_encoding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/page_block_navigator.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/block/page_block_navigator_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common/tensor_layout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common/tensor_layout.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/common.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/reduce/block/block_reduce.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/reduce/block/block_reduce_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_rand_uniform_kernel.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_rand_uniform_kernel_hip.h [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/stream_config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/stream_config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/hip_check_error.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/hip_check_error.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/timer.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/timer.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/env.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/env.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/ck.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/ck.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/integral_constant.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/integral_constant.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/enable_if.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/enable_if.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/number.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/number.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/math.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/math.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/sequence.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/sequence.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional2.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/functional2.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/tuple.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/tuple.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/statically_indexed_array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/statically_indexed_array.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/data_type.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/data_type.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/f8_utils.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/f8_utils.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/random_gen.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/random_gen.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/array.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/array.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type_convert.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/utility/type_convert.hpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_inner_product.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_inner_product.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_math_ext.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_math_ext.h [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/stream_config.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/stream_config.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/hip_check_error.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/hip_check_error.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/kernel_launch.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/host_utility/kernel_launch_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/tensor_operation/gpu/device/device_base.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck/tensor_operation/gpu/device/device_base.hpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_decoder.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_decoder.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_splitk.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_attention_forward_decoder_splitk_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_splitk.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_splitk.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_test.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_test.cu [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_bool_switch.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_bool_switch.h [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_bwd_setting.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_bwd_setting_hip.h [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/arg_parser.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/arg_parser.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/ranges.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/ranges.hpp [skipped, no changes]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/device_memory.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/device_memory_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/fill.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/fill_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/host_tensor.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/host_tensor_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_dropout.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_dropout_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_elementwise.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_elementwise_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_masking.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_masking_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_rotary_position_embedding.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_rotary_position_embedding_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_softmax.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_batched_softmax_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_gemm.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_gemm_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_im2col.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_im2col_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_layernorm2d.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_layernorm2d_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_reduce.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_reduce_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_softmax.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/reference/reference_softmax_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue/default_2d_epilogue.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue/default_2d_epilogue_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue.hpp -> /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/epilogue_hip.hpp [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_headdim_switch.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_headdim_switch_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_fwd_setting.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_fwd_setting_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_instances_ref.h -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_instances_ref_hip.h [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_bf16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_has_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_has_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_has_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_backward_fp16_no_causalmask_no_bias_no_biasgrad_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_forward_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_bf16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_has_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_has_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_has_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_128.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_256.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_32.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.cu -> /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_grouped_infer_fp16_no_causalmask_no_bias_no_dropout_maxk_64.hip [skipped, already hipified]
  /app/src/extras/xformers/xformers/csrc/attention/matmul.cpp -> /app/src/extras/xformers/xformers/csrc/attention/matmul.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/sddmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/sddmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/sparse_softmax.cpp -> /app/src/extras/xformers/xformers/csrc/attention/sparse_softmax.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/attention/spmm.cpp -> /app/src/extras/xformers/xformers/csrc/attention/spmm.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/memset_32b.cpp -> /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/memset_32b.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/synchronization.cpp -> /app/src/extras/xformers/xformers/csrc/sequence_parallel_fused/synchronization.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/sparse24/sparse24.cpp -> /app/src/extras/xformers/xformers/csrc/sparse24/sparse24.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_op.cpp -> /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_op.cpp [skipped, no changes]
  /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_packedw.cpp -> /app/src/extras/xformers/xformers/csrc/swiglu/swiglu_packedw.cpp [skipped, no changes]
  [92mSuccessfully preprocessed all matching files.[0m
  /tmp/.local/lib/python3.11/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.
  !!

          ********************************************************************************
          Please consider removing the following classifiers in favor of a SPDX license expression:

          License :: OSI Approved :: BSD License

          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
          ********************************************************************************

  !!
    self._finalize_license_expression()
  Total number of unsupported CUDA function calls: 0


  Total number of replaced kernel launches: 10
  running bdist_wheel
  running build
  running build_py
  creating build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/utils.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/info.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/test.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/attn_bias_utils.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/_deprecation_warning.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/checkpoint.py -> build/lib.linux-x86_64-cpython-311/xformers
  copying xformers/_cpp_lib.py -> build/lib.linux-x86_64-cpython-311/xformers
  creating build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/find_slowest.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/device_limits.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/profiler.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/profiler_dcgm.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/api.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/profile_analyzer.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  copying xformers/profiler/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn
  creating build/lib.linux-x86_64-cpython-311/xformers/components
  copying xformers/components/residual.py -> build/lib.linux-x86_64-cpython-311/xformers/components
  copying xformers/components/input_projection.py -> build/lib.linux-x86_64-cpython-311/xformers/components
  copying xformers/components/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components
  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_merge_attentions.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_core.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_indexing.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_sp24.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  copying xformers/benchmarks/benchmark_revnet.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks
  creating build/lib.linux-x86_64-cpython-311/xformers/triton
  copying xformers/triton/vararg_kernel.py -> build/lib.linux-x86_64-cpython-311/xformers/triton
  copying xformers/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/triton
  creating build/lib.linux-x86_64-cpython-311/xformers/sparse
  copying xformers/sparse/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse
  copying xformers/sparse/csr_tensor.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse
  copying xformers/sparse/blocksparse_tensor.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse
  copying xformers/sparse/_csr_ops.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse
  copying xformers/sparse/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse
  creating build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/sp24.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/differentiable_collectives.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/common.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/seqpar.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/tiled_matmul.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/indexing.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/modpar_layers.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/rmsnorm.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/unbind.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/swiglu_op.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/rope_padded.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  copying xformers/ops/ipc.py -> build/lib.linux-x86_64-cpython-311/xformers/ops
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers
  copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers
  copying xformers/_flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers
  copying xformers/_flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  copying xformers/_flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses
  copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses
  copying xformers/_flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/interface_torch.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  copying xformers/_flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  copying xformers/_flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  copying xformers/_flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  copying xformers/_flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  copying xformers/_flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  copying xformers/_flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  copying xformers/_flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  copying xformers/_flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  copying xformers/_flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  copying xformers/_flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  copying xformers/_flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  copying xformers/_flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  copying xformers/_flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops
  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton
  creating build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/attention_patterns.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/fourier_mix.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/attention_mask.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/core.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/scaled_dot_product.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/sparsity_config.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/base.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  copying xformers/components/attention/_sputnik_sparse.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention
  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA
  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code
  copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code
  copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code
  copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code
  creating build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/common.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/triton_splitk.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/torch_attention_compat.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/ck_decoder.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/dispatch.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/flash.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/cutlass.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/ck_splitk.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/attn_bias.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/ck.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  copying xformers/ops/fmha/flash3.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha
  creating build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/matmul_perf_model.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton
  creating build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton
  copying xformers/ops/fmha/_triton/splitk_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton
  copying xformers/ops/fmha/_triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton
  running build_ext
  building 'xformers._C' extension
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/instances
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24
  creating /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu
  [1/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [2/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [3/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [4/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [5/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_forward_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [6/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_infer_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [7/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_forward_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [8/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_infer_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [9/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/matmul.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/cpu/matmul.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/matmul.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [10/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/attention.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/attention.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/attention.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [11/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/spmm.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/cpu/spmm.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/spmm.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [12/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sddmm.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/cpu/sddmm.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sddmm.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [13/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/cpu/sparse_softmax.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [14/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_fmha_test.cu -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_fmha_test.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [15/481] g++ -MMD -MF /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd/matmul.o.d -march=znver5 -mtune=znver5 -O3 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512cd -mavx512vbmi -mavx512vbmi2 -mavx512vnni -mavx512bitalg -mavx512vpopcntdq -mavx512bf16 -pipe -fno-plt -fexceptions -flto=auto -fPIC -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/autograd/matmul.cpp -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd/matmul.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C
  [16/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip:20:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_rand_uniform_kernel_hip.h:9:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  8 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_ck_rand_uniform.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [17/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_decoder.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_forward_decoder.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [18/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_splitk.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_forward_splitk.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [19/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  fatal error: too many errors emitted, stopping now [-ferror-limit=]
  20 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [20/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdGroupModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:161:11: note: in instantiation of function template specialization 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, true, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    161 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_hip.h:357:14: note: in instantiation of member function 'grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    357 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_grouped_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_grouped_backward_causalmask_bias_dropout_dispatch<
        |               ^
  fatal error: too many errors emitted, stopping now [-ferror-limit=]
  20 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_grouped_backward_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [21/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<unsigned short, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  fatal error: too many errors emitted, stopping now [-ferror-limit=]
  20 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_bf16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [22/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 128, 8, 4>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1578:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 128, 32, 8, 4>' requested here
   1578 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kMPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:394:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeBiasLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
    394 |             bias_lds_ptr, Policy::template MakeBiasLdsBlockDescriptor<Problem>());
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<32>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 4, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<32>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, _Float16, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<32>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:12:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>, 32, 32, 4, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<_Float16, _Float16, _Float16, _Float16, float, float, float, _Float16, unsigned short, _Float16, _Float16, _Float16, _Float16, _Float16, _Float16, FmhaBwdShape<32>, false, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, _Float16, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip:30:15: note: in instantiation of function template specialization 'run_batched_backward_causalmask_bias_dropout_dispatch<_Float16, false, false, true, true, 32>' requested here
     30 |               run_batched_backward_causalmask_bias_dropout_dispatch<
        |               ^
  fatal error: too many errors emitted, stopping now [-ferror-limit=]
  20 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_fp16.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [23/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_backward_generic_ck_tiled.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [24/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/attention_forward_generic_ck_tiled.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  [25/481] /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  FAILED: [code=1] /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.o
  /opt/rocm-7.1.1/bin/hipcc  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip -o /app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.o -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 --offload-arch=gfx1151 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:11:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host_hip.hpp:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err_hip.hpp:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:10:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp:29:36: error: use of undeclared identifier 'CK_TILE_BUFFER_RESOURCE_3RD_DWORD'
     29 |     buffer_resource res{ptr, size, CK_TILE_BUFFER_RESOURCE_3RD_DWORD};
        |                                    ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:11:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host_hip.hpp:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err_hip.hpp:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:36:13: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     36 | #if defined(__AMDGCN_WAVEFRONT_SIZE__)
        |             ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:11:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host_hip.hpp:8:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/check_err_hip.hpp:17:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core_hip.hpp:11:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/core/arch/arch_hip.hpp:37:12: error: macro '__AMDGCN_WAVEFRONT_SIZE__' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Werror,-Wdeprecated-pragma]
     37 |     return __AMDGCN_WAVEFRONT_SIZE__;
        |            ^
  <built-in>:907:141: note: macro marked 'deprecated' here
    907 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE__, "compile-time-constant access to the wavefront size will be removed in a future release")
        |                                                                                                                                             ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1726:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1726 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<0>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1758:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1758 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<1>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1776:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1776 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<2>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1795:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1795 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<3>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1829:24: error: explicit specialization cannot have a storage class [-Werror,-Wexplicit-specialization-storage-class]
   1829 |         CK_TILE_DEVICE static constexpr void GemmStagedScheduler<4>()
        |                        ^~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>, 128, 16, 8, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::GetSmemSize' requested here
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>, 128, 128, 8, 16>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::GetSmemSize' requested here
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, true, true, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, true>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1113:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>, 128, 16, 8, 2>' requested here
   1113 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1601:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledQLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1601 |             MakeShuffledQLdsWriteBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1695:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeQT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1695 |         constexpr index_t smem_size_qt   = GetSmemSizeQT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: in instantiation of member function 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::GetSmemSize' requested here
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::GetSmemSize' requested here
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:729:31: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
    729 |                 ? KThreadRead / (kfold * K0PerThreadWrite / K0PerThreadRead)
        |                               ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:987:16: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeXTLdsBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>, 128, 128, 8, 16>' requested here
    987 |         return MakeXTLdsBlockDescriptor<Problem, kNPerBlock, kKPerBlock, kKPack, kKPackT>();
        |                ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:996:42: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeShuffledKLdsWriteBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
    996 |         auto shuffled_k_lds_block_desc = MakeShuffledKLdsWriteBlockDescriptor<Problem>();
        |                                          ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1620:13: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::MakeKTLdsReadBlockDescriptor<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1620 |             MakeKTLdsReadBlockDescriptor<Problem>().get_element_space_size();
        |             ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1698:44: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSizeKT<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
   1698 |         constexpr index_t smem_size_kt   = GetSmemSizeKT<Problem>();
        |                                            ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:83:33: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::GetSmemSize<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>' requested here
     83 |         return Policy::template GetSmemSize<Problem>();
        |                                 ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:580:43: note: (skipping 1 context in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    580 |         return ck_tile::max(FmhaPipeline::GetSmemSize(),
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:588:34: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::GetSmemSize' requested here
    588 |         __shared__ char smem_ptr[GetSmemSize()];
        |                                  ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1810:37: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1810 |                     ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE) > 0
        |                                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp:755:40: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdPipelineDefaultPolicy::HotLoopScheduler<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::GemmStagedScheduler<3>' requested here
    755 |             HotLoopScheduler::template GemmStagedScheduler<3>();
        |                                        ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp:1018:43: note: in instantiation of function template specialization 'ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>::operator()<ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<128>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::null_tile_window<ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 8, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, const float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::unmerge<ck_tile::tuple<int>, false>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>>, ck_tile::tuple<ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::sequence<2>, long, ck_tile::sequence<-1, 1, -1>, ck_tile::sequence<-1, 1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, float, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::pass_through<int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::atomic_add>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::tile_window_with_static_lengths<ck_tile::tensor_view<ck_tile::buffer_view<ck_tile::address_space_enum::global, unsigned short, long, true, ck_tile::amd_buffer_coherence_enum::coherence_default>, ck_tile::tensor_descriptor<ck_tile::tuple<ck_tile::embed<ck_tile::tuple<int, int>, ck_tile::tuple<int, int>>, ck_tile::right_pad<int, int>, ck_tile::right_pad<int, int>>, ck_tile::tuple<ck_tile::sequence<0>, ck_tile::sequence<1>, ck_tile::sequence<2>>, ck_tile::tuple<ck_tile::sequence<1, 2>, ck_tile::sequence<3>, ck_tile::sequence<4>>, ck_tile::sequence<3, 4>, long, ck_tile::sequence<-1, -1, 1, -1, -1>, ck_tile::sequence<-1, -1, 1, -1, -1>>, ck_tile::memory_operation_enum::set>, ck_tile::tuple<ck_tile::constant<16>, ck_tile::constant<128>>>, ck_tile::EmptyPositionEncoding<float>>' requested here
   1018 |         auto [dk_acc_tile, dv_acc_tile] = FmhaPipeline{}(q_dram_window,
        |                                           ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:21:5: note: in instantiation of member function 'ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::operator()' requested here
     21 |     Kernel{}(args...);
        |     ^
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/host/kernel_launch_hip.hpp:38:25: note: in instantiation of function template specialization 'ck_tile::kentry<128, 1, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>, ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>::FmhaBwdBatchModeKargs>' requested here
     38 |     const auto kernel = kentry<MaxThreadPerBlock, MinBlockPerCu, KernelImpl, Args...>;
        |                         ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:164:11: note: in instantiation of function template specialization 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::RunWithBwdDQDKDVKernel<ck_tile::FmhaBwdDQDKDVKernel<ck_tile::BlockFmhaBwdDQDKDVPipelineKRKTRVRIGLP<ck_tile::BlockFmhaBwdPipelineProblem<unsigned short, unsigned short, unsigned short, unsigned short, float, float, float, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, FmhaBwdShape<128>, false, false, ck_tile::SimplifiedGenericAttentionMask<>, ck_tile::BlockDropoutBwd<true, false, false>, ck_tile::TileFmhaTraits<true, true, false, false, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, true, false, false, false, 1>>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>, ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<float, unsigned short, true, false>>>>' requested here
    164 |           RunWithBwdDQDKDVKernel<FmhaBwdDQDKDVKernel_>(param, stream);
        |           ^
  /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:375:14: note: in instantiation of member function 'batched_backward_causalmask_bias_dropout_dispatch<unsigned short, true, true, true, true, 128>::Run' requested here
    375 |       MaxK>::Run(param, stream);
        |              ^
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip:13:
  In file included from /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/ck_tiled_fmha_batched_backward_hip.h:13:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha_hip.hpp:22:
  In file included from /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp:8:
  /app/src/extras/xformers/third_party/composable_kernel_tiled/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp:1811:43: error: division by zero is undefined [-Werror,-Wdivision-by-zero]
   1811 |                           ? LDS_READ_INST / (MFMA_INST - MFMA_INST_LDS_WRITE)
        |                                           ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  16 errors generated when compiling for gfx1151.
  failed to execute:/opt/rocm-7.1.1/lib/llvm/bin/clang++  --offload-arch=gfx1151  -I/app/src/extras/xformers/xformers/csrc -I/app/src/extras/xformers/xformers/csrc/attention/hip_fmha -I/app/src/extras/xformers/third_party/composable_kernel_tiled/include -I/tmp/.local/lib/python3.11/site-packages/torch/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/.local/lib/python3.11/site-packages/torch/include/THH -I/opt/rocm-7.1.1/include -I/usr/include/python3.11 -c -c -x hip /app/src/extras/xformers/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.hip -o "/app/src/extras/xformers/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/hip_fmha/instances/fmha_batched_backward_bf16_has_causalmask_has_bias_has_biasgrad_has_dropout_maxk_128.o" -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -O3 -std=c++17 -U__HIP_NO_HALF_OPERATORS__ -U__HIP_NO_HALF_CONVERSIONS__ -DCK_TILE_FMHA_FWD_FAST_EXP2=1 -fgpu-flush-denormals-to-zero -Werror -Woverloaded-virtual -mllvm -enable-post-misched=0 -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -mllvm -greedy-reverse-local-assignment=1 -DBUILD_PYTHON_PACKAGE -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -fno-gpu-rdc
  ninja: build stopped: subcommand failed.
  Traceback (most recent call last):
    File "/tmp/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2597, in _run_ninja_build
      subprocess.run(
    File "/usr/lib/python3.11/subprocess.py", line 571, in run
      raise CalledProcessError(retcode, process.args,
  subprocess.CalledProcessError: Command '['ninja', '-v', '-j', '25']' returned non-zero exit status 1.

  The above exception was the direct cause of the following exception:

  Traceback (most recent call last):
    File "/tmp/.local/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 389, in <module>
      main()
    File "/tmp/.local/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 373, in main
      json_out["return_val"] = hook(**hook_input["kwargs"])
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 280, in build_wheel
      return _build_backend().build_wheel(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/build_meta.py", line 435, in build_wheel
      return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/build_meta.py", line 423, in _build
      return self._build_with_temp_dir(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/build_meta.py", line 404, in _build_with_temp_dir
      self.run_setup()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/build_meta.py", line 512, in run_setup
      super().run_setup(setup_script=setup_script)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/build_meta.py", line 317, in run_setup
      exec(code, locals())
    File "<string>", line 684, in <module>
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/__init__.py", line 115, in setup
      return distutils.core.setup(**attrs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/core.py", line 186, in setup
      return run_commands(dist)
             ^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/core.py", line 202, in run_commands
      dist.run_commands()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/dist.py", line 1002, in run_commands
      self.run_command(cmd)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/dist.py", line 1102, in run_command
      super().run_command(command)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
      cmd_obj.run()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/command/bdist_wheel.py", line 370, in run
      self.run_command("build")
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py", line 357, in run_command
      self.distribution.run_command(command)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/dist.py", line 1102, in run_command
      super().run_command(command)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
      cmd_obj.run()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/command/build.py", line 135, in run
      self.run_command(cmd_name)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py", line 357, in run_command
      self.distribution.run_command(command)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/dist.py", line 1102, in run_command
      super().run_command(command)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
      cmd_obj.run()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/command/build_ext.py", line 96, in run
      _build_ext.run(self)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py", line 368, in run
      self.build_extensions()
    File "<string>", line 641, in build_extensions
    File "/tmp/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1082, in build_extensions
      build_ext.build_extensions(self)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py", line 484, in build_extensions
      self._build_extensions_serial()
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py", line 510, in _build_extensions_serial
      self.build_extension(ext)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/command/build_ext.py", line 261, in build_extension
      _build_ext.build_extension(self, ext)
    File "/usr/local/lib/python3.11/dist-packages/Cython/Distutils/build_ext.py", line 136, in build_extension
      super().build_extension(ext)
    File "/tmp/.local/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py", line 565, in build_extension
      objects = self.compiler.compile(
                ^^^^^^^^^^^^^^^^^^^^^^
    File "/tmp/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 866, in unix_wrap_ninja_compile
      _write_ninja_file_and_compile_objects(
    File "/tmp/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2223, in _write_ninja_file_and_compile_objects
      _run_ninja_build(
    File "/tmp/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2614, in _run_ninja_build
      raise RuntimeError(message) from e
  RuntimeError: Error compiling objects for extension
  error: subprocess-exited-with-error
  
  Ã— Building wheel for xformers (pyproject.toml) did not run successfully.
  â”‚ exit code: 1
  â•°â”€> No available output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /usr/bin/python3 /tmp/.local/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmpbtd9w7a4
  cwd: /app/src/extras/xformers
  Building wheel for xformers (pyproject.toml): finished with status 'error'
  ERROR: Failed building wheel for xformers
Failed to build xformers
ERROR: Failed to build one or more wheels
